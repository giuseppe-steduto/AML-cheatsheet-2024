\section*{Introduction \& recap}
\subsection*{Expectation}
$\mathbbm{E}[X]=\int_{\Omega}xf(x)\di x=\int_{\omega}x\mathbb{P}[X{=}x]\di x$ \\
$\mathbb{E}_{Y|X}[Y]=\mathbb{E}_{Y}[Y|X]$\\
$\mathbb{E}_{X,Y}[f(X,Y)]=\mathbb{E}_{X}\mathbb{E}_{Y|X}[f(X,Y)|X]$
% $\mathbb{E}_{Y|X}[f(X,Y)|X]{=}\int_\mathbb{R}f(X,y)\mathbb{P}(y|X)\di y$

\subsection*{Variance \& Covariance}
$\mathbb{V}(X){=}\mathbb{E}[(X{-}\mathbb{E}[X])^2]{=}\mathbb{E}[X^2]{-}\mathbb{E}[X]^2$\\
$\mathbb{V}[X{+}Y]{=}\mathrm{Var}[X]{+}\mathrm{Var}[Y] \quad \text{for }X,Y \,\text{iid} \\
\mathbb{V}(AX) = A \mathbb{V}(X) A^T$ \\
$\mathbb{V}[\alpha X]=\alpha^2\mathrm{Var}[X]$ \\
$\mathrm{Cov}(X,Y)=\mathbb{E}[(X-\mathbb{E}[X])(Y-\mathbb{E}[Y])]$

\subsection*{Conditional Probabilities \& Bayes}
$\mathbb{P}[X|Y]=\frac{\mathbb{P}[X,Y]}{\mathbb{P}[Y]}=\frac{\mathbb{P}[Y|X]\mathbb{P}[X]}{\mathbb{P}[Y]}$

\subsection*{Distributions \& Gaussians}
$\mathcal{N}(x|\mu, \sigma^2)=\frac{e^{-(x-\mu)^2/(2\sigma^2)}}{\sqrt{2\pi\sigma^2}}$\\
$\mathcal{N}(x|\bm{\mu}, \bm{\Sigma})= \frac{e^{-\frac{1}{2}(\mathbf{x}-\bm{\mu})^T\bm{\Sigma}^{-1}(\mathbf{x}-\bm{\mu})}}{(2\pi)^{D/2}|\bm{\Sigma}|^{1/2}} $\\
$\mathcal{N}_1\cdot\mathcal{N}_2=\mathcal{N}(x|\frac{\mu_1 \sigma_2^2 + \mu_2 \sigma_1^2}{\sigma_2^2 + \sigma_1^2}, \frac{\sigma_1^2 \sigma_2^2}{\sigma_1^2 + \sigma_2^2})$\\

$\text{Ber}(x|\theta) = \theta^x(1-\theta)^{1-x} \quad 0 \leq \theta \leq 1$
\subsection*{Cauchy-Schwarz}
$\big(\mathbb{E}[xy]\big)^2 \leqslant \mathbb{E}[x^2]\mathbb{E}[y^2]$
\subsection*{Jensen Inequality}
$f(\sum_{i=1}^n \lambda_i x_i) \leq \sum_{i=1}^n \lambda_i f(x_i)$
\subsection*{Markov's Inequality}
$P(X \geq a) \leq \frac{\mathbb{E}[X]}a$
\subsection*{Union Bound}
$P\big(\bigcup_i A_i\big) \leq \sum_iP(A_i)$

\subsection*{Derivatives \& Matrices}
$\frac{\partial}{\partial \mathbf{x}}(\mathbf{b}^\top \mathbf{x}) = \frac{\partial}{\partial \mathbf{x}}(\mathbf{x}^\top \mathbf{b}) = \mathbf{b}$ \\
$\frac{\partial}{\partial \mathbf{x}}(\mathbf{x}^\top \mathbf{x}) = 2\mathbf{x}$ \\
$\frac{\partial}{\partial \mathbf{x}}(\mathbf{x}^\top \mathbf{A}\mathbf{x}) = (\mathbf{A}^\top + \mathbf{A})\mathbf{x}$ \quad
$\frac{\partial}{\partial \mathbf{x}}(\mathbf{b}^\top \mathbf{A}\mathbf{x}) = \mathbf{A}^\top \mathbf{b}$ \quad
$\frac{\partial}{\partial \mathbf{X}}(\mathbf{c}^\top \mathbf{X} \mathbf{b}) = \mathbf{c}\mathbf{b}^\top$ \quad \\
$\frac{\partial}{\partial \mathbf{x}}(\|\mathbf{x}\|^2_2) = \frac{\partial}{\partial \mathbf{x}} (\mathbf{x}^\top \mathbf{x}) = 2\mathbf{x}$ \quad
$\frac{\partial}{\partial \mathbf{X}}(\|\mathbf{X}\|_F^2) = 2\mathbf{X}$  \quad \quad
$\frac{\partial}{\partial \mathbf{x}}||\mathbf{x}||_1 = \frac{\mathbf{x}}{|\mathbf{x}|}$ \\
$\frac{\partial}{\partial \mathbf{x}}(\|\mathbf{Ax - b}\|_2^2) = \mathbf{2(A^\top Ax-A^\top b)}$ \quad
$\frac{\partial}{\partial x}(\mathbf{Y}^{-1}) = -\mathbf{Y}^{-1}$ \\
$(\mathbf{AB})^{-1} = \mathbf{B}^{-1}\mathbf{A}^{-1}$ \\
Moore-P$\mathbf{A^+}$: $\mathbf{AA^+A} = \mathbf{A}\quad (\mathbf{A^+})^+=\mathbf{A}$
$\mathbf{A}\space $ positive definite $\iff \lambda_i>0\forall i$

\subsection*{Entropy and mutual information}
Entropy: $H(x)=-\int_{\mathcal{X}}p(x)\log p(x) dx$ is non-negative and max. when $\mathcal{X}$ is uniform \\
$H(X, Y)=H(X)+H(Y)$ if $X, Y$ indep. \\
$0 \leq H(X|Y) \leq H(X)$ \\
$I(X; Y) = I(Y; X) = H(X) - H(X|Y)$

\subsection*{Kullback-Leibler divergence}
$D_{KL}(p||q)=\mathbb{E}_{x\in\mathcal{X}}\big[\log\frac{p(x)}{q(x)}\big]\geq 0$ \\
$D_{KL}(p||q)=0\iff p=q$ \\
$CE(p||q) = -\sum_zp(z)(\log q(z))=$ [cross entropy] $= H(p) + D_{KL}(p||q)$ \textbf{Pr.} write $H(p) = +\sum p(x)\log\frac1{p(x)}$ e usa prop. log\\
