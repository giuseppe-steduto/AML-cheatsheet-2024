\section*{Ensemble Methods}
\subsection*{Combining Regressors}
Set of estimators: $\hat{f}_1(x), \cdots, \hat{f}_B(x)$\\
simple average: $\hat{f}(x) = \frac{1}{B}\sum_{i=1}^B\hat{f}_i(x)$
$\mathrm{Bias}[\hat{f}(x)]=\frac{1}{B}\sum_{i=1}^B\mathrm{Bias}[f_i(x)]$\\
$\mathbb{V}[\hat{f}(x)]\approx\frac{\sigma^2}{B}$ if the estimators are uncorrelated.

\subsection*{Combining Classifiers}
Input: classifiers $c_1(x),\cdots,c_B(x)$\\
Infer $\hat{c}_B(x){=}\text{sgn}(\sum_{b=1}^B\alpha_b c_b(x))$\\
with weights $\{\alpha_b\}_{b=1}^B $\\
Requires diversity of the classifiers.

\subsection*{Bagging}
Train on bootstrapped subsets.\\
Sample: $\mathcal{Z}=\{(x_1,y_1),\cdots(x_n,y_n)\}$\\
$\mathcal{Z}^*$: chose i.i.d from $\mathcal{Z}$ w. replacement.\\
Covariance small, variance similar, bias weakly affected.

 \subsection*{Random Forest (Bagging strategy)}
 Collection of uncorr. decision trees.
 Partition data space recursively. Grow the tree sufficiently deep to reduce bias. (each tree on other bagged set and with random collection of features available at every node)
 Prediction with voting.

\subsection*{Boosting}
Combine uncorr. weak learners in sequence. (Weak to avoid overfitting).\\
Coeff. of $\hat{c}_{b+1}$ depend on $\hat{c}_{b}$'s results\\
\textbf{AdaBoost} (minimizes exp. loss)\\
Init: $\mathcal{X}{=}\{(x_1,y_1),\cdots,(x_n,y_n)\}, w_i^{(1)}{=}\frac{1}{n}$\\
Fit  $\hat{c}_b(x)$ to $\mathcal{X}$ weighted by $w^{(b)}$\\
$\epsilon_b=\sum_{i=1}^nw_i^{(b)}\mathbb{I}_{\{\hat c_b(x_i)\not=y_i\}}/\sum_{i=1}^nw_i^{(b)}$\\
$\alpha_b = \mathrm{log}\frac{1-\epsilon_b}{\epsilon_b}>0$\\
$w_i^{(b+1)}=w_i^{(b)}\mathrm{exp}(\alpha_b\mathbb{I}_{\{{\hat c_b(x_i)\not=y_i}\}})$\\
return $\hat{c}_B(x){=}\mathrm{sgn}(\sum_{b{=}1}^B\alpha_b \hat c_b(x))$\\
Best approx. at log-odds ratio. \\
Like stagewise-additive modeling.

\subsection*{Difference}
(1) Boosting keeps identical training data, bagging potentially varies the training data for each classifier. (2) Boosting weighs the prediction of each classifier according to its accuracy, bagging gives same importance to each.

\subsection*{Notes}
AdaBoost gives large weight to samples that are hard to classify: those could be outliers. For bagging, there is a chance that imbalanced data-sets lead to bootstrap samples missing a class alltogether. Fix by making the bootstrap size large enough s.t. at least one point is included.
\subsection*{Logistic Regression}
$log\frac{P(y=1|x)}{P(y=-1|x)} = \sum_{b=1}^Bc_b(x) =: F(x)$
$P(y=1|x) = \frac{exp(F(x)}{1+exp(F(x))}$

%=========================================================
\section*{PAC Learning}
\subsection*{Risks}
\textbf{GenE:}$\mathcal{R}(\hat c)=\mathbb{P}(\hat{c}(X)\ne c(X))$ (unknown distribution, so compute ExpE)\\
\textbf{ExpE:}$\mathcal{\hat R}_n(\hat c)=\frac{1}{n}\sum^n_i1\{\hat c(x_i)\ne y_i\}$\\
ERM is $\hat c_{n}^*=\min_c \hat{\mathcal R_n}(c)$\\
ExpE is unbiased estimator of GenE
\subsection*{PAC Learning Model}
A learning algorithm $\mathcal A$ can learn a concept class $\mathcal C$ from an hypothesis class $\mathcal H$ if there is a polynomial function such that: (1) $\forall c\in\mathcal C$, (2) $\forall\mathcal D$ (train set), (3) $\forall 0<\epsilon,\delta<0.5$ then $\mathbb{P}(\mathcal R(c)\le\epsilon)\ge 1-\delta$. If $\mathcal A$ runs in $poly(1/\epsilon,1/\delta)$, Efficient PAC.\\
The universal class is not PAC-learnable
\textbf{Consistent:}$\mathcal{\hat R}_n(\hat c)=0$
\textbf{Pr. Err Finite Consistent:} Parti da $\mathbb{P}(\mathcal {\hat R}_n(\hat c)=0)\le (1-\epsilon)^n$ ma serve per tutte HPs. Hence $\le |\mathcal H|(1-\epsilon)^n\le|\mathcal H|e^{-n\epsilon}$ e $\hat c$ dato da $\mathcal A$.
\subsection*{General PAC - with noise}
$\mathbb{P}((\mathcal R(\hat c)-\inf_c R(c))\le\epsilon)\ge 1-\delta$\\
\textbf{Err Finite H:} $\mathbb{P}(R(\hat c_n^*)-\inf_cR(c)>\epsilon)\le 2|C|exp(-2n\epsilon^2)$
\textbf{Err Infinite H:} $\le 9n^{VC}exp(-n\epsilon^2/32)$
\textbf{VC Ineq:} $\mathcal{R}_n(\hat{c}_n^*)-\inf_{c\in\mathcal{C}}\mathcal{R}(c) \leq 2\sup_{c\in\mathcal{C}}\lvert \hat{\mathcal{R}}_n(c) - \mathcal{R}(c) \rvert$ \textbf{Pr.} Add $\mathcal +-R(\hat c_n^*)$, $\le$ using $c^*$ and group.
\subsection*{Hoeffding's Ineq}
If $\mathbb{E}[X]=0$, then $\mathbb{E}[e^{sX}]\le e^{\frac{s^2(b-a)^2}{8}}$.
\subsection*{Hoeffding's Th}
Let $X_{i\le n}\in[a_i,b_i]$ iid, and $S_n=\sum_i^nX_i$, then $\mathbb{P}(S_n-\mathbb{E}[S_n]>\epsilon)\le exp(\frac{-2\epsilon^2}{\sum_i^n(b_i-a_i)^2})$
\textbf{Err Finite Classif:} $\mathbb{P}(\sup_c|\mathcal R(c)-\hat R_n(c)|>\epsilon)\le 2Nexp(-2n\epsilon^2)$ \textbf{Pr.} Use UniBound e Hoeff Theo.

%=========================================================
\section*{Nonparametric Bayesian methods}
$\text{Beta}(x|a,b)=\beta(a,b)^{-1} x^{a-1}(1-x)^{b-1}$: prob. of Bernoulli proc. after observing $a-1$ success and $b-1$ failures. $\beta(a,b)=\frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}$. If $a,b$ enormi, approx Gauss.
Dirichlet Ã¨ Beta multivariata. E' conjugate prior. $Dir(x;\alpha)=\frac{1}{\beta(\alpha)}\prod_k x_k^{\alpha_k-1}$ con $\beta(\alpha)=\frac{\prod\Gamma(\alpha)}{\Gamma(\sum\alpha)}$.\\
\textbf{Dirich Proc:} For partitions of $\Theta$ $B_{i\le n}, G\sim DP(\alpha,G)$, $(G(B_1),..,G(B_n))\sim Dir(\alpha H(B_1),..,\alpha H(B_n))$ \textbf{Remark:} prob of two points being equal is 0 (G continuous)
\textbf{Stick-breaking proc} \\ Repeatedly draw from $\text{Beta}(x|1,\alpha)$ with fixed $\alpha$, but from reducing stick: $\rho_k=\beta_k(1-\sum_{i=1}^{k-1}\rho_i)$. The prior:\\
$\mathbb{P}[z_i=k|z_{-i},\alpha]=\begin{cases}\frac{N_{k,-i}}{\alpha+N-1} & \text{existing }k \\ \frac{\alpha}{\alpha+N-1} & \text{otherwise}\end{cases}$ Return is GEM($\alpha$). Larger $\alpha$, smaller first stick\\
Final Gibbs sampler:\\
$\mathbb{P}[z_i=k|z_{-i},\alpha,\mu]=\begin{cases}\frac{N_{k,-i}}{\alpha+N-1}p(x_i|x_{-i,k},\mu) & \text{existing }k \\ \frac{\alpha}{\alpha+N-1}p(x_i,\mu) & \text{otherwise}\end{cases}$

\subsection*{Gibbs sampling}
Init: assign all data to a cluster, with prior $\pi_i$, with $\sum_{k=1}^K\pi_i<1$ (s.t. new clusters possible). E.g. with stick-breaking. \\
Then remove $x$ from $k$ and compute new $\theta_k$, then compute Gibbs sampler prob. (CRP), and sample the new cluster assignment $z_i\sim p(z_i|x_{-i},\theta_k)$. If cluster is empty, \\
remove it and decrease $K$.